<!-- ---
header-includes:
    - \usepackage[makeroom]{cancel}
--- -->

# Matrizes

Uma matriz $m \times n$ tem $m$ linhas e $n$ colunas. Também é comum usarmos $i \times j$, e você pode encontrar essa notação. Chamamos isso de **Ordem** da matriz.

Chamamos uma matriz de quadrada se ela possuí número igual de linhas e colunas, isto é, se $m=n$

$$
M_{m\times n}=
\begin{bmatrix}
    a_{1,1} & a_{1,2} & \dots & a_{1,n} \\
    a_{2,1} & a_{2,2} & \dots & a_{2,n} \\
    \vdots  & \vdots & \ddots & \vdots \\
    a_{m,1} & a_{m,2} & \dots & a_{m_n}
\end{bmatrix}
$$

## Matriz Identidade

Uma matriz identidade é uma matriz quadrada com $1$s em sua diagonal e $0$ como outros elementos. É comum chamarmos a matriz identidade de ordem $n$ de $I_n$: 
$$
I_{1} = 
\begin{bmatrix}
1
\end{bmatrix}
$$

$$
I_{n} =
\begin{bmatrix}
    1 & 0 & \dots & 0 \\
    0 & 1 & \dots & 0 \\
    \vdots  & \vdots & \ddots & \vdots \\
    0 & 0 & \dots & 1_{(a_{n,n})}
\end{bmatrix}
$$

Esse nome, "identidade", fará mais sentido quando discutirmos multiplicação de matrizes.

## Soma e Subtração de Matrizes

Para somar matrizes, primeiro temos que garantir que elas possuem mesma ordem. Caso, por exmeplo possuam números de linhas e colunas diferentes entre si, não será possível somá-las.

Dessa forma, matrizes com mesma ordem, ou seja, mesmo número de linhas e colunas, podem ser somadas ou subtraídas:


$$
\begin{aligned}
    M_{i\times j}+N_{i\times j}&=
    \begin{bmatrix}
        a_{1,1} & a_{1,2} & \dots & a_{1,j} \\
        a_{2,1} & a_{2,2} & \dots & a_{2,j} \\
        \vdots  & \vdots & \ddots & \vdots \\
        a_{i,1} & a_{i,2} & \dots & a_{i,j}
    \end{bmatrix}
    +
    \begin{bmatrix}
        b_{1,1} & b_{1,2} & \dots & b_{1,j} \\
        b_{2,1} & b_{2,2} & \dots & b_{2,j} \\
        \vdots  & \vdots & \ddots & \vdots \\
        b_{mi1} & b_{i,2} & \dots & b_{i,j}
    \end{bmatrix}\\ \\
    &=
    \begin{bmatrix}
        a_{1,1} + b_{1,1} & a_{1,2} + b_{1,2} & \dots & a_{1,j} + b_{1,j} \\
        a_{2,1} + b_{2,1} & a_{2,2} + b_{2,2} & \dots & a_{2,j} + b_{2,j} \\
        \vdots  & \vdots & \ddots & \vdots \\
        a_{i,1} + b_{i,1} & a_{i,2} + b_{i,2} & \dots & a_{i,j} + b_{i,j}
    \end{bmatrix}
\end{aligned}
$$

## Multiplicação de matrizes por escalar
Chamamos de escalar um número (normalmente, real ou complexo, aqui chamado de $\lambda$) que multiplica um vetor ou matriz. Para multiplicar uma matriz por um escalar, multiplicamos todos seus elementos por ele, idependente de sua ordem:

$$
\lambda~ M_{m\times n}=
\lambda~
\begin{bmatrix}
    a_{1,1} & a_{1,2} & \dots & a_{1,n} \\
    a_{2,1} & a_{2,2} & \dots & a_{2,n} \\
    \vdots  & \vdots & \ddots & \vdots \\
    a_{m,1} & a_{m,2} & \dots & a_{m,n}
\end{bmatrix}
=
\begin{bmatrix}
    \lambda a_{1,1} & \lambda a_{1,2} & \dots & \lambda a_{1,n} \\
    \lambda a_{2,1} & \lambda a_{2,2} & \dots & \lambda a_{2,n} \\
    \vdots  & \vdots & \ddots & \vdots \\
    \lambda a_{m,1} & \lambda a_{m,2} & \dots & \lambda a_{m,n}
\end{bmatrix}
$$

## Multiplicação de Matrizes

Para multiplicarmos duas matrizes, é necessário que o número de colunas da primeira matriz seja igual ao número de linhas da segunda matriz. Por esse e outros motivos, dizemos que a multiplicação de matrizes *não é comutativa*, ou seja, multiplicar uma matriz $M$ por uma matriz $N$ pode nos dar uma matriz resultante diferente do que se multiplicarmos $N$ por $M$, caso essa multiplicação seja se quer possível!

$$
\begin{aligned}
    M_{i\times j} \times N_{j\times k}, j=j \Rightarrow \checkmark\\
    M_{i\times j} \times B_{k\times j}, j\neq k \Rightarrow \xcancel{\checkmark}\\
    N_{j\times k} \times M_{i\times j}. k\neq i \Rightarrow \xcancel{\checkmark}
\end{aligned}
$$

Vamos analisar como a operação é feita, e então nos ficará claro o porquê dessa regra existir.
sadasdsad
asdsa

Considere as seguintes matrizes:

$$
A_{2,3} =
\begin{bmatrix}
    1 & 2 & 3 \\
    4 & 5 & 6
\end{bmatrix},~
B_{3,1} = 
\begin{bmatrix}
    7 \\
    8 \\
    9
\end{bmatrix}
$$

Sabemos que podemos multiplicá-las com A como primeira matriz $A_{2,3}\times B_{3,1}, 3=3\Rightarrow \checkmark$, mas não como segunda matriz: $B_{3,1} \times A_{2,3}, 1\neq 2 \Rightarrow \xcancel{\checkmark}$. Iremos então realizar a primeira operação descrita da seguinte maneira:

*Definição.* Para multiplicar matrizes, somaremos cada linha da primeira multiplicada por um elemento equivalente de cada coluna:

$$
\begin{aligned}
    A \times B &= 
    \begin{bmatrix}
        1 & 2 & 3 \\
        4 & 5 & 6
    \end{bmatrix}
    \times 
    \begin{bmatrix}
        7 \\
        8 \\
        9
    \end{bmatrix}\\
    &=
    \begin{bmatrix}
        1 \cdot 7 + 2 \cdot 8 + 3 \cdot 9 \\
        4 \cdot 7 + 5 \cdot 8 + 6 \cdot 9
    \end{bmatrix} \\
    &=
    \begin{bmatrix}
        50 \\
        122
    \end{bmatrix}
\end{aligned}
$$

É importante que você se familiarize com o "pareamento" feito entre as linhas da primeira matriz com as linhas da segunda. Você pode agora estar se perguntando o que aconteceria caso houvesse mais de uma coluna na segunda matriz. A resposta pode ser bastante intuitiva para você: a matriz resultante terá mais uma coluna.

$$
\begin{aligned}
    &C \times D = 
    \begin{bmatrix}
        a & b & c \\
        d & e & f
    \end{bmatrix}
    \times 
    \begin{bmatrix}
        g & h \\
        i & j\\
        k & l
    \end{bmatrix}\\
    &=
    \begin{bmatrix}
        a \cdot g + b \cdot i + c \cdot k & a \cdot h + b \cdot j + c \cdot k\\
        d \cdot g + e \cdot i + f \cdot k & d \cdot h + e \cdot j + f \cdot k
    \end{bmatrix}
\end{aligned}
$$

Note que o número de linhas da matriz resultate da multiplicação entre matrizes é sempre igual ao número de linhas da primeira matriz e o de colunas igual ao da segunda.

------------------------------------------------------------------------

E onde a matriz identidade entra no jogo?

Para qualquer matriz $M_{i,j}$, 

$$
I_{i} \times M_{i,j} = M_{i,j} = M_{i,j}\times I_{j}
$$

Prove!

------------------------------------------------------------------------

A multiplicação de matrizes, por mais que simples, é extremamente poderosa e é a base por trás de importantes conceitos matemáticos. Um deles é a inversão de matriz, que você verá adiante.

## Determinantes
Deteminantes são computações especiais realizadas em matrizes quadradas. Possuem significado importante para conceitos de álgebra linear, mas, por hora, apenas aprenderemos como calculá-los para matrizes de até ordem 3.

O determinante de uma matriz de ordem 1 é, simplesmente, o valor nela contido:
$$
\det{A_{1\times1}}=
\begin{vmatrix}
    3
\end{vmatrix} = 3
$$

onde $||$ representa o determinante, e não o módulo, da matriz com único elemento 3.

E para matrizes de maior ordem?

Seja 
$$
M_{2\times2} =
\begin{bmatrix}
    1 & 2\\
    3 & 4 
\end{bmatrix}
$$

Podemos calcular o determinante de M, $\det{M},\det{(M)}, |M|$, multiplicando a diagonal principal e subtraindo do produto da outra diagonal:
$$
\det{(M)}=\begin{vmatrix}
    1 & 2\\
    3 & 4 
\end{vmatrix} = 1 \cdot 4 - (2 \cdot 3) = -2
$$

E quais seriam as diagonais de uma matriz de ordem 3?

Seja
$$
N_{3\times3} =
\begin{bmatrix}
    1 & 2 & 3\\
    4 & 5 & 6\\
    7 & 8 & 9
\end{bmatrix}
$$

Podemos calcular seu determinante com algumas técnicas. A mais comum forma de lembramos desse método é a chamada *Fórmula de Leibniz para Determinantes*. Assim como nas matrizes de ordem 2, iremos somar os produtos das diagonais "principais" (esquerda para direita) e subtrair os produtos das outras diagonais (direita para esquerda). Para visualizarmos tais diagonais "escondidas" na matriz, usaremos a *Regra de Sarrus*: copiar as primeiras duas colunas no final:

$$
\begin{aligned}
    |N_{3\times3}| &=
    \left|\begin{array}{ccc|cc}
        1 & 2 & 3 & 1 & 2 \\ 
        4 & 5 & 6 & 4 & 5 \\
        7 & 8 & 9 & 7 & 8
    \end{array}\right| \\
    &= (1 \cdot 5 \cdot 9 + 2 \cdot 6 \cdot 7 + 3 \cdot 4 \cdot 8) \\
    &- (2 \cdot 4 \cdot 9 + 1 \cdot 6 \cdot 8 + 3 \cdot 5 \cdot 7)\\
    &= 225 - 225 = 0
\end{aligned}
$$

Existem outras formas de calcular determinantes. Para os leitores interesados, recomendamos que busquem a resolução de determinantes pelo *Teorema de Laplace*, também conhecido como expansão de cofatores. Esse é um poderoso teorema nos permite calcular determinantes de matrizes de ordem maior do que 3, além de também poder aplicado na matriz 3x3 para (algumas vezes) cálculos mais simples.

## Inversão de matrizes
Dizemos que uma matriz quadrada $M_{n\times n}$ é inversível se existe uma outra matriz, $N$, tal que:
$$
    M\times N = N \times M = I_n
$$

### Escalonamento

Uma das formas de chegamos na matriz inversa é considerarmos os seguinte:
$N$ pode ser escrita como o produto de outras matrizes.

$$
\begin{cases}
    M \times N = N \times M = I_{n} \\
    N = {A \times B}
\end{cases}\Rightarrow M \times (A \times B) = (A\times B) \times M = I_n 
$$

> Note que os parênteses aqui são desnecessários: por mais que a multiplicação de matrizes não seja comutativa, ela é associativa! (Recomendamos que prove essa propriedade).

Sabemos também que, pela propriedade da matriz identidade,

$$
A \times B = A \times B \times I_n = I_n \times A \times B
$$

Com esse recurso, podemos chegar na matriz inversa através de operações mais simples partindo de uma matriz como a identidade!

Antes de prosseguirmos, note uma interessante propriedade: *Uma matriz é inversível se, e somente se, seu determinante for diferente de $0$. A razão disso não ficará clara agora, mas leitores interessados podem se referir à @sec-material.


## Material adicional {#sec-material}

Para os curiosos, este vídeos sobre [Multiplicação de Matrizes](https://youtu.be/XkY2DOUCWMU), este sobre [Determinantes](https://www.youtube.com/watch?v=Ip3X9LOh2dk) e este sobre [Matrizes Inversas e a Identidade](https://youtu.be/uQhTuRlWMxw) do canal no YouTube [3blue1brown](https://www.youtube.com/@3blue1brown) (em inglês, com legendas em português) te darão uma intuição sobre o que estamos fazendo de fato quando multiplicamos matrizes.